# -*- coding: utf-8 -*-
"""Copy of employee burnout analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15i5jK0xVyKSy21ZMiZ3ydrthSnVifnts

Importing Libraries
"""

import warnings
warnings.filterwarnings('ignore')

import pandas as pd
import numpy as np

import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px

from google.colab import drive
drive.mount('/content/drive')

"""Loading Dataset"""

pd.set_option('display.max_columns', None)
burnoutDf=pd.read_csv('/content/drive/MyDrive/burnout. csv')
burnoutDf

# convert into dateTime dataType
burnoutDf["Date of Joining"]= pd.to_datetime(burnoutDf["Date of Joining"])

# give the number of rows and columns
burnoutDf.shape

# general information
burnoutDf.info()

# show top 5 rows
burnoutDf.head()

# extract all columns of the dataset
burnoutDf.columns

#check for null values
burnoutDf.isna().sum()

# check the duplicate values
burnoutDf.duplicated().sum()

# calculate the mean , std, min, max and count of every attributes
burnoutDf.describe()

# show the unique values
for i, col in enumerate(burnoutDf.columns):
  print(f"\n\n{burnoutDf[col].unique()}")
  print(f"\n{burnoutDf[col].value_counts()}\n\n")

# Drop irrelevant column
burnoutDf=burnoutDf.drop(['Employee ID'],axis=1)

# check the skewness of the attributes
intFloatburnoutDf=burnoutDf.select_dtypes([np.int, np.float])
for i, col in enumerate(intFloatburnoutDf.columns):
  if (intFloatburnoutDf[col].skew() >= 0.1):
    print("\n",col, "feature is Positively Skewed and value is: ", intFloatburnoutDf[col].skew())
  elif (intFloatburnoutDf[col].skew() <= -0.1):
      print("\n",col, "feature is Negatively Skewed and value is: ", intFloatburnoutDf[col].skew())
  else:
        print("\n",col, "feature is Normally Distributed and value is: ", intFloatburnoutDf[col].skew())

# Replace the null values with mean
burnoutDf['Resource Allocation'].fillna(burnoutDf['Resource Allocation'].mean(),inplace=True)
burnoutDf['Mental Fatigue Score'].fillna(burnoutDf['Mental Fatigue Score'].mean(),inplace=True)
burnoutDf['Burn Rate'].fillna(burnoutDf['Burn Rate'].mean(),inplace=True)

# check for null values
burnoutDf.isna().sum()

# show the correlation
burnoutDf.corr()

"""Data Visualization"""

# Plotting Heat map to check correlation
corr=burnoutDf.corr()
sns.set(rc={'figure.figsize':(14,12)})
fig = px.imshow(corr, text_auto=True, aspect="auto")
fig.show()

# Count plot Distribution of "Gender"
plt.figure(figsize=(10,8))
sns.countplot(x="Gender", data=burnoutDf, palette="magma")
plt.title("plot Distribution of Gender")
plt.show()

# Count plot distribution of "Company Type"
plt.figure(figsize=(10,8))
sns.countplot(x="Company Type", data=burnoutDf, palette="Spectral")
plt.title("plot distribution of Company Type")
plt.show()

# Count plot distribution of "WFH Setup Available"
plt.figure(figsize=(10,8))
sns.countplot(x="WFH Setup Available", data=burnoutDf, palette="dark:salmon_r")
plt.title("plot distribution of WFH_Setup_Availble")
plt.show()

# Count-plot diaatribution of attributes with the help of Histogram
burn_st=burnoutDf.loc[:,'Date of Joining':'Burn Rate']
burn_st=burn_st.select_dtypes([int, float])
for i, col in enumerate(burn_st.columns):
    fig = px.histogram(burn_st, x=col, title="Plot Distribution of "+col, color_discrete_sequence=['indianred'])
    fig.update_layout(bargap=0.2)
    fig.show()

# Plot distribution of Burn rate on the basis of Designation
fig = px.line(burnoutDf, y="Burn Rate", color="Designation", title="Burn rate on the basis of Designation",color_discrete_sequence=px.colors.qualitative.Pastel1)
fig.update_layout(bargap=0.1)
fig.show()

# Plot distribution of Burn Rate on the basis of Gender
fig = px.line(burnoutDf, y="Burn Rate", color="Gender", title="Burn Rate on the basis of Gender",color_discrete_sequence=px.colors.qualitative.Pastel1)
fig.update_layout(bargap=0.2)
fig.show()

# Plot distribution of mental fatigue score on the basis of Designation
fig = px.line(burnoutDf, y="Mental Fatigue Score", color="Designation", title="Mental Fatigue vs Designation",color_discrete_sequence=px.colors.qualitative.Pastel1)
fig.update_layout(bargap=0.2)
fig.show()

# plot distribution of "Designation vs mental fatigue"as per Company type , Burn rate and Gender
sns.relplot(
    data=burnoutDf, x="Designation", y="Mental Fatigue Score", col="Company Type",
    hue="Company Type", size="Burn Rate", style="Gender",
    palette=["g", "r"], sizes=(50, 200)
)

"""Label Encoding"""

# label encoding and assign in new variable
from sklearn import preprocessing
Lable_encode = preprocessing.LabelEncoder()

# assign in new variable
burnoutDf['GenderLable'] = Lable_encode.fit_transform(burnoutDf['Gender'].values)
burnoutDf['Company_TypeLable'] = Lable_encode.fit_transform(burnoutDf['Company Type'].values)
burnoutDf['WFH_Setup_AvailableLable'] = Lable_encode.fit_transform(burnoutDf['WFH Setup Available'].values)

# check assigned values
gn = burnoutDf.groupby('Gender')
gn = gn['GenderLable']
gn.first()

# check assigned values
ct = burnoutDf.groupby('Company Type')
ct = ct['Compant_TypeLabel']
ct.first()

# check assigned values
wsa = burnoutDf.groupby('WFH Setup Available')
wsa = wsa['WFH_Setup_AvailableLable']
wsa.first()

# show last 10 rows
burnoutDf.tail(10)

"""Feature Selection"""

# feature selection
columns=['Designation', 'Resource Allocation', 'Mental Fatigue Score',
       'GenderLable', 'Company_TypeLable', 'WFH_Setup_Available']
x=burnoutDf[columns]
y=burnoutDf['Burn Rate']

print(x)

print(y)

"""Implementing PCA"""

# principle component analysis
from sklearn.decomposition import PCA
pca = PCA(0.95)
x_pca = pca.fit_transform(x)
print("PCA shaoe of x is: ",x_pca.shape, "and original shape is: ", x.shape)
print("% of importance of selected features is:", pca.explained_variance_ratio_)
print("The number of features selected through PCA is:", pca.n_components_)

"""Data Splitting"""

# Data Splitting in train and test
from sklearn.model_selection import train_test_split
x_train_pca, x_test, v_train, v_test = train_test_split(x_pca,y, test_size = 0.25, random_state=10)

# print the shape of splitted data
print(x_train_pca.shape, x_test.shape, v_train.shape, v_test.shape)

"""MODEL IMPLEMENTATION

Random Forest Regressor
"""

from sklearn.metrics import r2_score

# Random Forest Regressor
from sklearn.ensemble import RandomForestRegressor

rf_model = RandomForestRegressor()
rf_model.fit(x_train_pca, v_train)

train_pred_rf = rf_model.predict(x_train_pca)
train_r2 = r2_score(v_train, train_pred_rf)
test_pred_rf = rf_model.predict(x_test)
test_r2 = r2_score(v_test, test_pred_rf)
# Accuracy score
print("Accuracy score of train data: "+str(round(100*train_r2, 4))+" %")
print("Accuracy score of the test data: "+str(round(100*test_r2, 4))+" %")

"""AdaBoost Regressor"""

# AdaBoost regressor
from sklearn.ensemble import AdaBoostRegressor
abr_model = AdaBoostRegressor()
abr_model.fit(x_train_pca, v_train)

train_pred_adboost = abr_model.predict(x_train_pca)
train_r2 = r2_score(v_train, train_pred_adboost)
test_pred_adaboost = abr_model.predict(x_test)
test_r2 = r2_score(v_test, test_pred_adaboost)

# Accuracy score
print("Accuracy score of train data: "+str(round(100*train_r2, 4))+" %")
print("Accuracy score of the test data: "+str(round(100*test_r2, 4))+" %")

"""BURNOUT PREDICTION"""

import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso

import warnings
warnings.filterwarnings(action='ignore')

burnoutDf=pd.read_csv('/content/drive/MyDrive/burnout. csv')

burnoutDf

burnoutDf.info()

def preprocess_inputs(df):
    df = df.copy()

    # Drop Employee ID column
    df = df.drop('Employee ID', axis=1)

    # Drop rows with missing target values
    missing_target_rows = df.loc[df['Burn Rate'].isna(), :].index
    df = df.drop(missing_target_rows, axis=0).reset_index(drop=True)

    # Fill remaining missing values with column means
    for column in ['Resource Allocation', 'Mental Fatigue Score']:
        df[column] = df[column].fillna(df[column].mean())

    # Extract date features
    df['Date of Joining'] = pd.to_datetime(df['Date of Joining'])
    df['Join Month'] = df['Date of Joining'].apply(lambda x: x.month)
    df['Join Day'] = df['Date of Joining'].apply(lambda x: x.day)
    df = df.drop('Date of Joining', axis=1)

    # Binary encoding
    df['Gender'] = df['Gender'].replace({'Female': 0, 'Male': 1})
    df['Company Type'] = df['Company Type'].replace({'Product': 0, 'Service': 1})
    df['WFH Setup Available'] = df['WFH Setup Available'].replace({'No': 0, 'Yes': 1})

    # Split df into X and y
    y = df['Burn Rate']
    X = df.drop('Burn Rate', axis=1)

    # Train-test split
    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True, random_state=1)

    # Scale X
    scaler = StandardScaler()
    scaler.fit(X_train)
    X_train = pd.DataFrame(scaler.transform(X_train), index=X_train.index, columns=X_train.columns)
    X_test = pd.DataFrame(scaler.transform(X_test), index=X_test.index, columns=X_test.columns)

    return X_train, X_test, y_train, y_test

X_train, X_test, y_train, y_test = preprocess_inputs(burnoutDf)

X_train

y_train

models = {
    "                     Linear Regression": LinearRegression(),
    " Linear Regression (L2 Regularization)": Ridge(),
    " Linear Regression (L1 Regularization)": Lasso(),
    "                         Decision Tree": DecisionTreeRegressor()
}

for name, model in models.items():
    model.fit(X_train, y_train)
    print(name + " trained.")

for name, model in models.items():
    print(name + " R^2 Score: {:.5f}".format(model.score(X_test, y_test)))